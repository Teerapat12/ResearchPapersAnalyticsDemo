{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>Word2Vec on Research papers</h2>\n",
    "<p>In this notebook, we will see how word2vec perform in research papers.</p>\n",
    "<p>Word2Vec tries to create a vector for each word. And we want two vector to be close to each other if they have similar semantics.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#Import packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import timeit\n",
    "import codecs\n",
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup # $ pip install beautifulsoup4\n",
    "from wordcloud import WordCloud\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleaned_paper_df = pd.read_csv('../dataset/cleaned_papers_pdf.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1042, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>content</th>\n",
       "      <th>directory</th>\n",
       "      <th>isValid</th>\n",
       "      <th>faculty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice Sharp</td>\n",
       "      <td>Discarded appendicularian houses as sources of...</td>\n",
       "      <td>../papers/BIO/Alice Sharp/0014.pdf</td>\n",
       "      <td>True</td>\n",
       "      <td>BIO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alice Sharp</td>\n",
       "      <td>AbstractThailandhassufferedfromseveredefor- es...</td>\n",
       "      <td>../papers/BIO/Alice Sharp/00463529693fc4921900...</td>\n",
       "      <td>True</td>\n",
       "      <td>BIO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alice Sharp</td>\n",
       "      <td>Cell, Vol. 11, 263-271, June 1977. Copyright 0...</td>\n",
       "      <td>../papers/BIO/Alice Sharp/0deec5230a91912b3800...</td>\n",
       "      <td>True</td>\n",
       "      <td>BIO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alice Sharp</td>\n",
       "      <td>ImprovingthesolidwastemanagementinPhnomPenhcit...</td>\n",
       "      <td>../papers/BIO/Alice Sharp/1-s2.0-S0956053X0400...</td>\n",
       "      <td>True</td>\n",
       "      <td>BIO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alice Sharp</td>\n",
       "      <td>arXiv:1001.2574v2 [astro-ph.CO] 26 Jan 2010Pro...</td>\n",
       "      <td>../papers/BIO/Alice Sharp/1001.2574</td>\n",
       "      <td>True</td>\n",
       "      <td>BIO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name                                            content  \\\n",
       "0  Alice Sharp  Discarded appendicularian houses as sources of...   \n",
       "1  Alice Sharp  AbstractThailandhassufferedfromseveredefor- es...   \n",
       "2  Alice Sharp  Cell, Vol. 11, 263-271, June 1977. Copyright 0...   \n",
       "3  Alice Sharp  ImprovingthesolidwastemanagementinPhnomPenhcit...   \n",
       "4  Alice Sharp  arXiv:1001.2574v2 [astro-ph.CO] 26 Jan 2010Pro...   \n",
       "\n",
       "                                           directory isValid faculty  \n",
       "0                 ../papers/BIO/Alice Sharp/0014.pdf    True     BIO  \n",
       "1  ../papers/BIO/Alice Sharp/00463529693fc4921900...    True     BIO  \n",
       "2  ../papers/BIO/Alice Sharp/0deec5230a91912b3800...    True     BIO  \n",
       "3  ../papers/BIO/Alice Sharp/1-s2.0-S0956053X0400...    True     BIO  \n",
       "4                ../papers/BIO/Alice Sharp/1001.2574    True     BIO  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(cleaned_paper_df.shape)\n",
    "cleaned_paper_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import various modules for string cleaning\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_wordlist( paper_content ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", paper_content) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))      \n",
    "    stops.add('cid')\n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops] \n",
    "    \n",
    "    # 5. Cut word with only 1 character except a,i,u\n",
    "  \n",
    "    meaningful_words = [w for w in meaningful_words if len(w)!=1 or w in['a','i','u']]\n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words ))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download the punkt tokenizer for sentence splitting  \n",
    "import nltk\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "# Define a function to split a review into parsed sentences\n",
    "def paper_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( review_to_wordlist( raw_sentence))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n",
      "Finished parsing\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print(\"Parsing sentences from training set\")\n",
    "for paper in cleaned_paper_df[\"content\"]:\n",
    "    sentences += paper_to_sentences(paper, tokenizer)\n",
    "print(\"Finished parsing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split_sentences = [sentence.split(\" \") for sentence in sentences if len(sentence)>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "2017-06-24 15:12:03,360 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2017-06-24 15:12:03,463 : INFO : collecting all words and their counts\n",
      "2017-06-24 15:12:03,465 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-06-24 15:12:03,518 : INFO : PROGRESS: at sentence #10000, processed 115196 words, keeping 36812 word types\n",
      "2017-06-24 15:12:03,571 : INFO : PROGRESS: at sentence #20000, processed 232029 words, keeping 44138 word types\n",
      "2017-06-24 15:12:03,621 : INFO : PROGRESS: at sentence #30000, processed 351003 words, keeping 61914 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-24 15:12:03,691 : INFO : PROGRESS: at sentence #40000, processed 480732 words, keeping 95649 word types\n",
      "2017-06-24 15:12:03,755 : INFO : PROGRESS: at sentence #50000, processed 600294 words, keeping 128127 word types\n",
      "2017-06-24 15:12:03,817 : INFO : PROGRESS: at sentence #60000, processed 725490 words, keeping 169099 word types\n",
      "2017-06-24 15:12:03,879 : INFO : PROGRESS: at sentence #70000, processed 832531 words, keeping 199099 word types\n",
      "2017-06-24 15:12:03,921 : INFO : PROGRESS: at sentence #80000, processed 957250 words, keeping 222723 word types\n",
      "2017-06-24 15:12:03,972 : INFO : PROGRESS: at sentence #90000, processed 1025047 words, keeping 246354 word types\n",
      "2017-06-24 15:12:04,056 : INFO : PROGRESS: at sentence #100000, processed 1147692 words, keeping 273990 word types\n",
      "2017-06-24 15:12:04,150 : INFO : PROGRESS: at sentence #110000, processed 1284256 words, keeping 311518 word types\n",
      "2017-06-24 15:12:04,242 : INFO : PROGRESS: at sentence #120000, processed 1416723 words, keeping 337142 word types\n",
      "2017-06-24 15:12:04,329 : INFO : PROGRESS: at sentence #130000, processed 1536656 words, keeping 356930 word types\n",
      "2017-06-24 15:12:04,379 : INFO : PROGRESS: at sentence #140000, processed 1663155 words, keeping 365140 word types\n",
      "2017-06-24 15:12:04,438 : INFO : PROGRESS: at sentence #150000, processed 1798774 words, keeping 406071 word types\n",
      "2017-06-24 15:12:04,486 : INFO : PROGRESS: at sentence #160000, processed 1918183 words, keeping 420295 word types\n",
      "2017-06-24 15:12:04,593 : INFO : PROGRESS: at sentence #170000, processed 2061135 words, keeping 442421 word types\n",
      "2017-06-24 15:12:04,656 : INFO : PROGRESS: at sentence #180000, processed 2188264 words, keeping 459019 word types\n",
      "2017-06-24 15:12:04,708 : INFO : PROGRESS: at sentence #190000, processed 2313218 words, keeping 462587 word types\n",
      "2017-06-24 15:12:04,755 : INFO : PROGRESS: at sentence #200000, processed 2442903 words, keeping 468154 word types\n",
      "2017-06-24 15:12:04,819 : INFO : PROGRESS: at sentence #210000, processed 2572572 words, keeping 479490 word types\n",
      "2017-06-24 15:12:04,874 : INFO : PROGRESS: at sentence #220000, processed 2702079 words, keeping 487999 word types\n",
      "2017-06-24 15:12:04,921 : INFO : PROGRESS: at sentence #230000, processed 2833145 words, keeping 491711 word types\n",
      "2017-06-24 15:12:04,968 : INFO : PROGRESS: at sentence #240000, processed 2941897 words, keeping 496385 word types\n",
      "2017-06-24 15:12:05,022 : INFO : PROGRESS: at sentence #250000, processed 3024901 words, keeping 500935 word types\n",
      "2017-06-24 15:12:05,086 : INFO : PROGRESS: at sentence #260000, processed 3148281 words, keeping 516344 word types\n",
      "2017-06-24 15:12:05,149 : INFO : PROGRESS: at sentence #270000, processed 3300136 words, keeping 524162 word types\n",
      "2017-06-24 15:12:05,202 : INFO : PROGRESS: at sentence #280000, processed 3424330 words, keeping 528322 word types\n",
      "2017-06-24 15:12:05,259 : INFO : PROGRESS: at sentence #290000, processed 3564085 words, keeping 533719 word types\n",
      "2017-06-24 15:12:05,316 : INFO : PROGRESS: at sentence #300000, processed 3711859 words, keeping 536842 word types\n",
      "2017-06-24 15:12:05,357 : INFO : collected 539862 word types from a corpus of 3796973 raw words and 305881 sentences\n",
      "2017-06-24 15:12:05,358 : INFO : Loading a fresh vocabulary\n",
      "2017-06-24 15:12:08,145 : INFO : min_count=0 retains 539862 unique words (100% of original 539862, drops 0)\n",
      "2017-06-24 15:12:08,146 : INFO : min_count=0 leaves 3796973 word corpus (100% of original 3796973, drops 0)\n",
      "2017-06-24 15:12:10,027 : INFO : deleting the raw counts dictionary of 539862 items\n",
      "2017-06-24 15:12:10,062 : INFO : sample=0.001 downsamples 3 most-common words\n",
      "2017-06-24 15:12:10,064 : INFO : downsampling leaves estimated 3795905 word corpus (100.0% of prior 3796973)\n",
      "2017-06-24 15:12:10,066 : INFO : estimated required memory for 539862 words and 500 dimensions: 2429379000 bytes\n",
      "2017-06-24 15:12:12,620 : INFO : resetting layer weights\n",
      "2017-06-24 15:12:24,014 : INFO : training model with 4 workers on 539862 vocabulary and 500 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-06-24 15:12:24,015 : INFO : expecting 305881 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-06-24 15:12:25,152 : INFO : PROGRESS: at 0.78% examples, 130715 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-24 15:12:26,284 : INFO : PROGRESS: at 1.83% examples, 149709 words/s, in_qsize 6, out_qsize 1\n",
      "2017-06-24 15:12:27,299 : INFO : PROGRESS: at 2.90% examples, 164491 words/s, in_qsize 6, out_qsize 1\n",
      "2017-06-24 15:12:28,327 : INFO : PROGRESS: at 4.15% examples, 176364 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-24 15:12:29,382 : INFO : PROGRESS: at 5.36% examples, 184615 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-24 15:12:30,401 : INFO : PROGRESS: at 6.68% examples, 188002 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-24 15:12:31,462 : INFO : PROGRESS: at 7.85% examples, 192065 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-24 15:12:32,467 : INFO : PROGRESS: at 8.91% examples, 192896 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-24 15:12:33,491 : INFO : PROGRESS: at 10.10% examples, 196318 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-24 15:12:34,510 : INFO : PROGRESS: at 11.01% examples, 195315 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-24 15:12:35,584 : INFO : PROGRESS: at 12.23% examples, 197844 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-24 15:12:36,586 : INFO : PROGRESS: at 13.39% examples, 200353 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-24 15:12:37,604 : INFO : PROGRESS: at 14.59% examples, 202994 words/s, in_qsize 8, out_qsize 1\n",
      "2017-06-24 15:12:38,634 : INFO : PROGRESS: at 16.09% examples, 205796 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-24 15:12:39,698 : INFO : PROGRESS: at 17.47% examples, 207748 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-24 15:12:40,786 : INFO : PROGRESS: at 18.67% examples, 209679 words/s, in_qsize 6, out_qsize 1\n",
      "2017-06-24 15:12:41,815 : INFO : PROGRESS: at 19.88% examples, 212635 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-24 15:12:42,826 : INFO : PROGRESS: at 21.19% examples, 213945 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-24 15:12:43,851 : INFO : PROGRESS: at 22.36% examples, 213962 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-24 15:12:44,884 : INFO : PROGRESS: at 23.66% examples, 214864 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-24 15:12:45,933 : INFO : PROGRESS: at 24.97% examples, 215059 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-24 15:12:46,960 : INFO : PROGRESS: at 26.56% examples, 216313 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-24 15:12:47,986 : INFO : PROGRESS: at 27.73% examples, 217048 words/s, in_qsize 8, out_qsize 1\n",
      "2017-06-24 15:12:49,003 : INFO : PROGRESS: at 29.05% examples, 218203 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-24 15:12:50,062 : INFO : PROGRESS: at 30.33% examples, 218922 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-24 15:12:51,115 : INFO : PROGRESS: at 31.47% examples, 219263 words/s, in_qsize 6, out_qsize 1\n",
      "2017-06-24 15:12:52,130 : INFO : PROGRESS: at 32.76% examples, 220222 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-24 15:12:53,158 : INFO : PROGRESS: at 34.08% examples, 221347 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-24 15:12:54,181 : INFO : PROGRESS: at 35.31% examples, 222123 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-24 15:12:55,193 : INFO : PROGRESS: at 36.95% examples, 222888 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-24 15:12:56,217 : INFO : PROGRESS: at 38.11% examples, 223512 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-24 15:12:57,292 : INFO : PROGRESS: at 39.38% examples, 224379 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-24 15:12:58,301 : INFO : PROGRESS: at 40.63% examples, 225053 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-24 15:12:59,302 : INFO : PROGRESS: at 42.01% examples, 225751 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-24 15:13:00,406 : INFO : PROGRESS: at 43.21% examples, 225204 words/s, in_qsize 8, out_qsize 1\n",
      "2017-06-24 15:13:01,429 : INFO : PROGRESS: at 44.55% examples, 225454 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-24 15:13:02,542 : INFO : PROGRESS: at 46.21% examples, 225414 words/s, in_qsize 6, out_qsize 1\n",
      "2017-06-24 15:13:03,545 : INFO : PROGRESS: at 47.36% examples, 225760 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-24 15:13:04,556 : INFO : PROGRESS: at 48.50% examples, 225548 words/s, in_qsize 8, out_qsize 1\n",
      "2017-06-24 15:13:05,571 : INFO : PROGRESS: at 49.69% examples, 225810 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-24 15:13:06,584 : INFO : PROGRESS: at 50.89% examples, 225832 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-24 15:13:07,652 : INFO : PROGRESS: at 52.13% examples, 226019 words/s, in_qsize 8, out_qsize 1\n",
      "2017-06-24 15:13:08,675 : INFO : PROGRESS: at 53.39% examples, 226433 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-24 15:13:09,695 : INFO : PROGRESS: at 54.59% examples, 226619 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-24 15:13:10,718 : INFO : PROGRESS: at 55.99% examples, 226792 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-24 15:13:11,726 : INFO : PROGRESS: at 57.41% examples, 227234 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-24 15:13:12,748 : INFO : PROGRESS: at 58.63% examples, 227767 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-24 15:13:13,818 : INFO : PROGRESS: at 59.79% examples, 228057 words/s, in_qsize 6, out_qsize 1\n",
      "2017-06-24 15:13:14,820 : INFO : PROGRESS: at 61.05% examples, 228276 words/s, in_qsize 8, out_qsize 1\n",
      "2017-06-24 15:13:15,873 : INFO : PROGRESS: at 62.37% examples, 228259 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-24 15:13:16,973 : INFO : PROGRESS: at 63.60% examples, 227857 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-24 15:13:18,026 : INFO : PROGRESS: at 64.92% examples, 227665 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-24 15:13:19,072 : INFO : PROGRESS: at 66.52% examples, 227872 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-24 15:13:20,099 : INFO : PROGRESS: at 67.43% examples, 227079 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-24 15:13:21,125 : INFO : PROGRESS: at 68.53% examples, 226672 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-24 15:13:22,187 : INFO : PROGRESS: at 69.72% examples, 226654 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-24 15:13:23,307 : INFO : PROGRESS: at 70.87% examples, 226078 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-24 15:13:24,351 : INFO : PROGRESS: at 72.06% examples, 226139 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-24 15:13:25,391 : INFO : PROGRESS: at 73.22% examples, 226036 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-24 15:13:26,398 : INFO : PROGRESS: at 74.37% examples, 226072 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-24 15:13:27,457 : INFO : PROGRESS: at 75.75% examples, 226234 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-24 15:13:28,499 : INFO : PROGRESS: at 77.26% examples, 226581 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-24 15:13:29,581 : INFO : PROGRESS: at 78.57% examples, 226932 words/s, in_qsize 8, out_qsize 1\n",
      "2017-06-24 15:13:30,600 : INFO : PROGRESS: at 79.68% examples, 227211 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-24 15:13:31,612 : INFO : PROGRESS: at 80.93% examples, 227349 words/s, in_qsize 8, out_qsize 1\n",
      "2017-06-24 15:13:32,667 : INFO : PROGRESS: at 82.34% examples, 227637 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-24 15:13:33,682 : INFO : PROGRESS: at 83.57% examples, 227618 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-24 15:13:34,689 : INFO : PROGRESS: at 84.95% examples, 227762 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-24 15:13:35,703 : INFO : PROGRESS: at 86.49% examples, 227880 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-24 15:13:36,730 : INFO : PROGRESS: at 87.60% examples, 227822 words/s, in_qsize 8, out_qsize 1\n",
      "2017-06-24 15:13:37,750 : INFO : PROGRESS: at 88.91% examples, 228057 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-24 15:13:38,770 : INFO : PROGRESS: at 90.10% examples, 228016 words/s, in_qsize 8, out_qsize 1\n",
      "2017-06-24 15:13:39,790 : INFO : PROGRESS: at 91.34% examples, 228372 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-24 15:13:40,797 : INFO : PROGRESS: at 92.64% examples, 228629 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-24 15:13:41,807 : INFO : PROGRESS: at 93.89% examples, 228869 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-24 15:13:42,820 : INFO : PROGRESS: at 95.05% examples, 228842 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-24 15:13:43,841 : INFO : PROGRESS: at 96.74% examples, 229294 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-24 15:13:44,859 : INFO : PROGRESS: at 97.90% examples, 229370 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-24 15:13:45,861 : INFO : PROGRESS: at 99.06% examples, 229468 words/s, in_qsize 6, out_qsize 1\n",
      "2017-06-24 15:13:46,599 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-06-24 15:13:46,663 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-06-24 15:13:46,679 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-06-24 15:13:46,688 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-06-24 15:13:46,690 : INFO : training on 18984865 raw words (18979601 effective words) took 82.6s, 229758 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 500    # Word vector dimensionality                      \n",
    "min_word_count = 0  # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(split_sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "539862"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-24 15:13:46,788 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('metabolic', 0.9642347097396851),\n",
       " ('strength', 0.9590809941291809),\n",
       " ('variation', 0.9545866847038269),\n",
       " ('swelling', 0.952375590801239),\n",
       " ('degradation', 0.9511069059371948),\n",
       " ('absorption', 0.9497958421707153),\n",
       " ('contrast', 0.9449074268341064),\n",
       " ('effect', 0.9436883926391602),\n",
       " ('induced', 0.9416981339454651),\n",
       " ('muscle', 0.9411588907241821)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('resistance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('svg', 0.9527514576911926),\n",
       " ('server', 0.9471654891967773),\n",
       " ('documents', 0.9339300394058228),\n",
       " ('files', 0.9294983148574829),\n",
       " ('xml', 0.9168329238891602),\n",
       " ('experiencedphysical', 0.9134659171104431),\n",
       " ('sql', 0.9124547243118286),\n",
       " ('textual', 0.9110561609268188),\n",
       " ('wikipedia', 0.9107706546783447),\n",
       " ('code', 0.9089975953102112)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('transcription', 0.9303451180458069),\n",
       " ('texts', 0.9281502366065979),\n",
       " ('reference', 0.9207703471183777),\n",
       " ('vocabularies', 0.9181904196739197),\n",
       " ('annotated', 0.9172554612159729),\n",
       " ('emsim', 0.9154324531555176),\n",
       " ('terminology', 0.9154263734817505),\n",
       " ('analyzed', 0.9152184724807739),\n",
       " ('unicode', 0.9140470027923584),\n",
       " ('manually', 0.9102402925491333)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('build', 0.9631944298744202),\n",
       " ('allow', 0.9628151059150696),\n",
       " ('able', 0.9508962035179138),\n",
       " ('fully', 0.9496178030967712),\n",
       " ('realize', 0.9479504227638245),\n",
       " ('acquire', 0.9470875263214111),\n",
       " ('communicate', 0.9468263387680054),\n",
       " ('creating', 0.9456331729888916),\n",
       " ('allowing', 0.943172812461853),\n",
       " ('offers', 0.9429171681404114)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('create')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('solar', 0.9141300320625305),\n",
       " ('distortion', 0.8976895213127136),\n",
       " ('fluid', 0.8933480381965637),\n",
       " ('amplifier', 0.8919662833213806),\n",
       " ('generator', 0.8908932209014893),\n",
       " ('shock', 0.8896439671516418),\n",
       " ('solid', 0.8887043595314026),\n",
       " ('field', 0.8871155977249146),\n",
       " ('conversion', 0.8853572010993958),\n",
       " ('load', 0.8853031992912292)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('electric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ontology', 0.9500008225440979),\n",
       " ('lexical', 0.9480582475662231),\n",
       " ('linguistic', 0.9404067397117615),\n",
       " ('identification', 0.9396563172340393),\n",
       " ('specification', 0.9362311363220215),\n",
       " ('representation', 0.9292150139808655),\n",
       " ('metadata', 0.9251999258995056),\n",
       " ('generic', 0.9235600829124451),\n",
       " ('concepts', 0.9214292168617249),\n",
       " ('spatial', 0.920572817325592)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('semantic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tests', 0.9566447138786316),\n",
       " ('measurements', 0.9369489550590515),\n",
       " ('experiments', 0.9170331954956055),\n",
       " ('withclinical', 0.9133439064025879),\n",
       " ('performed', 0.9131519198417664),\n",
       " ('tested', 0.9116257429122925),\n",
       " ('experimental', 0.9044852256774902),\n",
       " ('experiment', 0.9030770063400269),\n",
       " ('suitability', 0.900475800037384),\n",
       " ('indcol', 0.8983445167541504)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('technological', 0.9409782886505127),\n",
       " ('upgrading', 0.9409516453742981),\n",
       " ('entrepreneurial', 0.9324290752410889),\n",
       " ('innovations', 0.9310087561607361),\n",
       " ('capability', 0.9307618737220764),\n",
       " ('nnovation', 0.9198551177978516),\n",
       " ('stakeholder', 0.9146895408630371),\n",
       " ('achievement', 0.9092878699302673),\n",
       " ('cultural', 0.9010015726089478),\n",
       " ('mpwt', 0.9005976319313049)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('innovation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
